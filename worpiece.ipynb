{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece guideline\n",
    "\n",
    "* Init vocab :\n",
    "    * Characters at the beginning of each word\n",
    "    * Characters in each word preceded by the prefix '##'\n",
    "    * Special tokens\n",
    "    * Compute pairs in each word with their respective frequency\n",
    "\n",
    "* Objects : \n",
    "    * Vocab : list containing the vocab\n",
    "    * Splits : dictionary containing pairs (updated) with their frequency in each word\n",
    "\n",
    "* Iteration : \n",
    "    * Merge pairs with the highest score (arbitrary choice equality)\n",
    "    * Until size_vocab reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Review\n",
    "**Defaultdict** : Python dictionnary which values can be initialized even if key not already present <br>\n",
    "**set.update** : Insert multiple items at once in a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "def init_vocab(text: List[str]) -> Tuple[list, list, list]:\n",
    "    special_tokens = ['[CLS]','[SEP]','[PAD]','[UNK]','[MASK]']\n",
    "    vocab = set()\n",
    "    word_freq = dd(int)\n",
    "    splits = dd(list)\n",
    "\n",
    "    for sentence in text:\n",
    "        words = sentence.split(\" \")\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "            if word not in splits:\n",
    "                splits[word] = [word[0]] + [f\"##{c}\" for c in word[1:]]\n",
    "                vocab.update(splits[word])\n",
    "\n",
    "    return special_tokens + list(vocab), word_freq, splits\n",
    "\n",
    "\n",
    "vocab, word_freq, splits = init_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict # for typing purposes\n",
    "\n",
    "def compute_init_scores(splits: dict) -> defaultdict:\n",
    "    # TODO : iterate through words to find pair_freq and individual_freq and compute scores for each pair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
