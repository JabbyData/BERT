{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# linear algebra\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(L: int, d_model: int, N: int = 10000) -> np.array:\n",
    "    pos = np.arange(L)[:, np.newaxis] # [L,1]\n",
    "    i = np.arange(d_model)[np.newaxis, :] # [1,d_model]\n",
    "\n",
    "    angle_rates = 1 / np.power(N, (2*(i//2)) / d_model)\n",
    "    angle_rads = pos * angle_rates # [L,d_model]\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    return angle_rads\n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=64):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.requires_grad = False\n",
    "\n",
    "        pe += positional_encoding(max_len, d_model)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0) # extra batch dimension : [1, 64, 128]\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.pe\n",
    "    \n",
    "\n",
    "class BERTEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0) # padding, seqA, seqB\n",
    "        self.position = PositionalEmbedding(embed_size,max_len)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, seq: list, segment_label: list):\n",
    "        embs = self.token(seq) + self.position() + self.segment(segment_label)\n",
    "        return self.dropout(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(torch.nn.Module):\n",
    "    def __init__(self,d_model: int, d_k: int,dropout_rate=0.1):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "\n",
    "        self.query = torch.nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        self.key = torch.nn.Linear(d_model, d_k)\n",
    "        self.values = torch.nn.Linear(d_model, d_k)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, E: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # E : [batch_size, max_len, d_model]\n",
    "\n",
    "        # Projection to attention space\n",
    "        Q_emb = self.query(E)\n",
    "        K_emb = self.key(E)\n",
    "        V_emb = self.values(E)\n",
    "        # [batch_size, max_len, d_k]\n",
    "\n",
    "\n",
    "        dk = self.query.out_features\n",
    "\n",
    "        # Scores\n",
    "        scores = torch.matmul(Q_emb, K_emb.transpose(-2,-1)) / np.sqrt(dk)\n",
    "        # [batch_size, max_len, max_len]\n",
    "        \n",
    "        # Attention weights\n",
    "        # Softmax on the deeper dimension of scores\n",
    "        attention_scores = F.softmax(scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        # [batch_size, max_len, max_len]\n",
    "\n",
    "        # Innovations\n",
    "        attention_values = torch.matmul(attention_scores,V_emb)\n",
    "        # [batch_size, max_len, d_model]\n",
    "\n",
    "        return attention_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve the process with parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # [d_model, n_heads * d_k]\n",
    "        self.query = torch.nn.Linear(d_model, d_model)\n",
    "        self.key = torch.nn.Linear(d_model, d_model)\n",
    "        self.value = torch.nn.Linear(d_model, d_model)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, emb):\n",
    "\n",
    "        # Projection to attention space\n",
    "        # [batch_size, max_len, d_model]\n",
    "        query_emb = self.query(emb)\n",
    "        key_emb = self.key(emb)\n",
    "        value_emb = self.value(emb)\n",
    "\n",
    "        # Parallelization\n",
    "        # [batch_size, n_heads, max_len, d_k]\n",
    "        batch_size = query_emb.shape[0]\n",
    "        query_emb = query_emb.view(batch_size, -1, self.n_heads, self.d_k).permute(0,2,1,3)\n",
    "        key_emb = key_emb.view(batch_size, -1, self.n_heads, self.d_k).permute(0,2,1,3)\n",
    "        value_emb = value_emb.view(batch_size, -1, self.n_heads, self.d_k).permute(0,2,1,3)\n",
    "\n",
    "        # Scores => similarity scores between tokens\n",
    "        # [batch_size, h, max_len, max_len]\n",
    "        attention_scores = torch.matmul(query_emb, key_emb.transpose(-2,-1)) / np.sqrt(self.d_k)\n",
    "\n",
    "        # Weights => part of each token that will update other tokens\n",
    "        # [batch_size, h, max_len, max_len]\n",
    "        weights = F.softmax(attention_scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # Innovations\n",
    "        # [batch_size, h, max_len, d_model]\n",
    "        innovations = torch.matmul(weights, value_emb)\n",
    "\n",
    "        # Concatenation \n",
    "        # [batch_size, max_len, d_model]\n",
    "        innovations = innovations.permute(0,2,1,3).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
    "\n",
    "        # Context to update embeddings\n",
    "        context = self.output_linear(innovations)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_ff, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_linear = torch.nn.Linear(d_model,d_ff)\n",
    "        self.output_linear = torch.nn.Linear(d_ff,d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.activation = torch.nn.GELU() # GELU performs better than RELU\n",
    "\n",
    "    def forward(self,emb):\n",
    "        \n",
    "        # Projection to hidden space\n",
    "        # [batch_size, max_len, d_ff]\n",
    "        output_hidden = self.hidden_linear(emb)\n",
    "        output_hidden = self.activation(output_hidden)\n",
    "\n",
    "        # Projection to original space\n",
    "        # [batch_size, max_len, d_model]\n",
    "        output_ff = self.output_linear(output_hidden)\n",
    "        output_ff = self.dropout(output_ff)\n",
    "\n",
    "        return output_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff,dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model,n_heads, dropout_rate)\n",
    "        self.layernorm = torch.nn.LayerNorm(normalized_shape=d_model)\n",
    "        self.ff = FeedForward(d_ff,d_model, dropout_rate)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, emb):\n",
    "        \n",
    "        # Linear context\n",
    "        # [batch_size, max_len, d_model]\n",
    "        output_mha = self.dropout(self.mha(emb))\n",
    "        updated_emb = self.layernorm(emb + output_mha)\n",
    "\n",
    "        # Context improved to non linear relationships\n",
    "        # [batch_size, max_len, d_model]\n",
    "        output_ff = self.ff(updated_emb)\n",
    "\n",
    "        encoder_output = self.layernorm(output_ff)\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, n_layers, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.encoder_layers = torch.nn.ModuleList(\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout_rate) for _ in range(n_layers)\n",
    "        )\n",
    "\n",
    "    def forward(self, emb):\n",
    "        encoder_output = emb\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output)\n",
    "\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Couple of tests\n",
    "\n",
    "batch_size = 3\n",
    "n_heads = 4\n",
    "max_len = 64\n",
    "d_model = 32\n",
    "d_k = d_model // n_heads # 8\n",
    "d_ff = 128\n",
    "n_layers = 2\n",
    "\n",
    "embs = torch.randn(size=(batch_size,max_len,d_model))\n",
    "\n",
    "sha = SingleHeadAttention(d_model,d_k)\n",
    "\n",
    "output_sha = sha(embs)\n",
    "\n",
    "output_sha.shape\n",
    "\n",
    "mha = MultiHeadAttention(d_model,n_heads)\n",
    "\n",
    "output_mha = mha(embs)\n",
    "\n",
    "output_mha.shape == torch.Size([batch_size, max_len, d_model])\n",
    "\n",
    "ff = FeedForward(d_ff, d_model)\n",
    "\n",
    "output_ff = ff(output_mha)\n",
    "\n",
    "output_ff.shape == torch.Size([batch_size, max_len, d_model])\n",
    "\n",
    "enc_l = EncoderLayer(d_model, n_heads, d_ff)\n",
    "\n",
    "output_enc_l = enc_l(embs)\n",
    "\n",
    "output_enc_l.shape == torch.Size([batch_size, max_len, d_model])\n",
    "\n",
    "enc = Encoder(d_model,n_heads,d_ff,n_layers)\n",
    "\n",
    "enc_output = enc(embs)\n",
    "\n",
    "enc_output.shape == torch.Size([batch_size, max_len, d_model])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
