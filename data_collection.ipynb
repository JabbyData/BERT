{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Load Dataset\n",
    "- Simple dataset for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 55/55 [00:00<00:00, 2053.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_dataset = load_dataset(\"javalove93/sentiment-analysis-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = sentiment_dataset['train']['text'][0]\n",
    "\n",
    "# Split by both '.' and '!'\n",
    "sentences = re.split(r'[.!]', text)\n",
    "sentences[:-1]\n",
    "\n",
    "sentiment_dataset['train']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching label strategy**\n",
    "- Split each sequence into single sentence.\n",
    "- Regrouping sentences with same label to create pairs : focus on relationship between them !\n",
    "- Here 2 sentiments : positive or negative\n",
    "- Pairs should be list of tuples or list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I love this movie!', \" It's amazing.\"),\n",
       " (\" It's amazing.\", 'What a great experience!'),\n",
       " ('What a great experience!', ' Highly recommended.'),\n",
       " (' Highly recommended.', \"This is the best book I've ever read.\"),\n",
       " (\"This is the best book I've ever read.\", \"I'm so happy with my purchase!\"),\n",
       " (\"I'm so happy with my purchase!\", 'I had a fantastic time!'),\n",
       " ('I had a fantastic time!', 'Absolutely loved it!'),\n",
       " ('Absolutely loved it!', 'This is incredible!'),\n",
       " ('This is incredible!', \"I'm very impressed with the performance.\"),\n",
       " (\"I'm very impressed with the performance.\", \"I can't wait to try it again!\"),\n",
       " (\"I can't wait to try it again!\", 'Excellent service and friendly staff.'),\n",
       " ('Excellent service and friendly staff.',\n",
       "  'Highly satisfied with the results.'),\n",
       " ('Highly satisfied with the results.', 'This is a must-see!'),\n",
       " ('This is a must-see!', 'It was a wonderful evening.'),\n",
       " ('It was a wonderful evening.', 'I highly recommend this service.'),\n",
       " ('I highly recommend this service.', 'Great value for money!'),\n",
       " ('Great value for money!', \"I'm so glad I bought this!\"),\n",
       " (\"I'm so glad I bought this!\", 'This is a fantastic product!'),\n",
       " ('This is a fantastic product!', \"I'm very happy with this service.\"),\n",
       " (\"I'm very happy with this service.\",\n",
       "  'I would definitely recommend this to others.'),\n",
       " ('I would definitely recommend this to others.', 'I had a great experience.'),\n",
       " ('I had a great experience.', 'This is a great place to visit.'),\n",
       " ('This is a great place to visit.', 'I love this!'),\n",
       " ('I love this!', 'This is amazing!'),\n",
       " ('This is amazing!', \"I'm so happy!\"),\n",
       " (\"I'm so happy!\", 'This is so good!'),\n",
       " ('This is so good!', \"I'm impressed!\"),\n",
       " (\"I'm impressed!\", 'This is perfect!'),\n",
       " ('This is perfect!', \"I'm thrilled!\"),\n",
       " (\"I'm thrilled!\", 'This is excellent!'),\n",
       " ('This is excellent!', \"I'm delighted!\"),\n",
       " ('This restaurant is terrible.', ' The food was cold.'),\n",
       " (' The food was cold.', 'The product arrived damaged.'),\n",
       " ('The product arrived damaged.', \" I'm very disappointed.\"),\n",
       " (\" I'm very disappointed.\", 'The service was slow and the staff were rude.'),\n",
       " ('The service was slow and the staff were rude.',\n",
       "  'This is a complete waste of money.'),\n",
       " ('This is a complete waste of money.',\n",
       "  'The quality is poor and it broke after a week.'),\n",
       " ('The quality is poor and it broke after a week.',\n",
       "  'I would not recommend this to anyone.'),\n",
       " ('I would not recommend this to anyone.',\n",
       "  'The instructions were unclear and confusing.'),\n",
       " ('The instructions were unclear and confusing.',\n",
       "  \"This is by far the worst experience I've had.\"),\n",
       " (\"This is by far the worst experience I've had.\",\n",
       "  'The product did not meet my expectations.'),\n",
       " ('The product did not meet my expectations.',\n",
       "  \"I'm extremely disappointed with the quality.\"),\n",
       " (\"I'm extremely disappointed with the quality.\",\n",
       "  'I regret buying this product.'),\n",
       " ('I regret buying this product.', 'The food was bland and tasteless.'),\n",
       " ('The food was bland and tasteless.',\n",
       "  'The delivery was late and the item was damaged.'),\n",
       " ('The delivery was late and the item was damaged.',\n",
       "  'The customer service was terrible.'),\n",
       " ('The customer service was terrible.', \"I'm not satisfied with the product.\"),\n",
       " (\"I'm not satisfied with the product.\",\n",
       "  \"This is the worst movie I've ever seen.\"),\n",
       " (\"This is the worst movie I've ever seen.\", 'I hate this!'),\n",
       " ('I hate this!', 'This is terrible!'),\n",
       " ('This is terrible!', \"I'm so sad.\"),\n",
       " (\"I'm so sad.\", 'This is so bad!'),\n",
       " ('This is so bad!', \"I'm disappointed.\"),\n",
       " (\"I'm disappointed.\", 'This is awful!'),\n",
       " ('This is awful!', \"I'm frustrated.\"),\n",
       " (\"I'm frustrated.\", 'This is unacceptable.'),\n",
       " ('This is unacceptable.', \"I'm angry.\")]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple # for cool decorations of functions\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Splitting initial sequences\n",
    "def create_sequences(dataset: datasets.Dataset) -> Tuple[list, list]:\n",
    "    sequences = dataset['train']\n",
    "    seq_pos = []\n",
    "    seq_neg = []\n",
    "    for sequence,label in zip(sequences['text'],sequences['label']):\n",
    "        sentences = re.split(r'([.!])',sequence)\n",
    "        sentences = [sentences[i] + sentences[i+1] for i in range(0, len(sentences)-1,2)]\n",
    "        if len(sentences[-1]) == 0:\n",
    "            sentences = [s[:MAX_LEN] for s in sentences[:-1]] # remove last sep empty\n",
    "        if label == 'positive':\n",
    "            seq_pos += sentences\n",
    "        else:\n",
    "            seq_neg += sentences\n",
    "    return seq_pos, seq_neg\n",
    "\n",
    "seq_pos, seq_neg = create_sequences(sentiment_dataset)\n",
    "\n",
    "# Generating pairs\n",
    "def generate_pairs(sequences: list) -> list:\n",
    "    return [(s1,s2) for s1,s2 in zip(sequences[:-1],sequences[1:])]\n",
    "\n",
    "pairs_pos = generate_pairs(seq_pos)\n",
    "pairs_neg = generate_pairs(seq_neg)\n",
    "\n",
    "sentences = seq_pos + seq_neg\n",
    "pairs = pairs_pos + pairs_neg\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Tokenization\n",
    "- Using WordPiece tokenizer to produce BERT inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:00<00:00, 36634.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/antonin/miniconda3/envs/tfm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1924: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizer\n",
    "# Creating batches\n",
    "batch_size = 30\n",
    "\n",
    "\n",
    "def create_batches(batch_size : int,sentences : list):\n",
    "    text_data = []\n",
    "    file_count = 0\n",
    "    for word in tqdm.tqdm(sentences):\n",
    "\n",
    "        text_data.append(word)\n",
    "\n",
    "        if len(text_data) == batch_size:\n",
    "            with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as file:\n",
    "                file.write('\\n'.join(text_data))\n",
    "            text_data = []\n",
    "            file_count += 1\n",
    "\n",
    "    with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as file:\n",
    "                file.write('\\n'.join(text_data))\n",
    "\n",
    "create_batches(batch_size,sentences)\n",
    "\n",
    "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n",
    "\n",
    "# Training the tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer.train( \n",
    "    files=paths,\n",
    "    vocab_size=30_000, \n",
    "    min_frequency=5,\n",
    "    limit_alphabet=1000, \n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
    "    )\n",
    "\n",
    "# os.mkdir('./bert-it-1')\n",
    "tokenizer.save_model('./bert-it-1', 'bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n",
    "\n",
    "#### TODO : review tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
